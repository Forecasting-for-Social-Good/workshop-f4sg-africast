---
title: "Lab exercise: day 5"
editor: visual
---

```{r}
#| label: setup

library(tsibble)
library(tsibbledata)
library(fable)
library(feasts)
library(tidyverse)
```

# Learn

Compare the in-sample accuracy statistics for all models used to forecast Australia's print media turnover. Which is most accurate, and which is least? Does this align with your expectations?

```{r}
aus_print <- aus_retail |> 
  filter(Industry == "Newspaper and book retailing") |> 
  summarise(Turnover = sum(Turnover))

aus_print |> 
  autoplot(Turnover)

fit <- aus_print |> 
  model(
    snaive = SNAIVE(Turnover),
    lm = TSLM(log(Turnover) ~ trend(knots = yearmonth("2011 Jan")) + season()),
    ets = ETS(Turnover),
    arima = ARIMA(log(Turnover))
  )

???(fit)
```

Modify the code to produce and evaluate forecasts on a 2-year test set. Does this change your conclusions?

```{r}
fit_train <- aus_print |> 
  ??? |> 
  model(
    snaive = SNAIVE(Turnover),
    lm = TSLM(log(Turnover) ~ trend(knots = yearmonth("2011 Jan")) + season()),
    ets = ETS(Turnover),
    arima = ARIMA(log(Turnover))
  )

fit_train |> 
  forecast(h = ???) |> 
  ???(???)
```

Now compare the cross-validated accuracy statistics for all models used to forecast Australia's print media turnover. Which is most accurate, and which is least? Does it differ to the results from the in-sample (training set) and out-of-sample (test set) accuracy?

```{r}
fit_cv <- aus_print |> 
  ??? |> 
  model(
    snaive = SNAIVE(Turnover),
    lm = TSLM(log(Turnover) ~ trend(knots = yearmonth("2011 Jan")) + season()),
    ets = ETS(Turnover),
    arima = ARIMA(log(Turnover))
  )

fit_cv |>
  forecast(h = "1 year") |>
  ???(???)
```

Produce a residual diagnostics plot of the ETS and ARIMA models, are the residuals consistent with the assumptions of the models? Do they appear random (time plot), uncorrelated (ACF plot), and normally distributed (histogram)?

```{r}
fit <- aus_print |> 
  model(
    ets = ETS(Turnover),
    arima = ARIMA(log(Turnover))
  )

fit |> 
  select(???) |> 
  gg_???()

fit |> 
  select(???) |> 
  gg_???()
```

# Apply

```{r}
vaccine_administrated_tsb <- read_rds("data/vaccine_administrated_tsb.rds")
```

## Basic of train/test forecast accuracy

### Split data

We tart by splitting data into two sets to describe the modeling process. We leave out 12 periods (equal to forecast horizon) as test set, and we pretend this is the future we want to forecast.

```{r}
forecast_horizon <- 12# forecast horizon
test <- vaccine_administrated_tsb |> filter(month >= (max(month)-forecast_horizon+1))
train <- vaccine_administrated_tsb |> filter(month < (max(month)-forecast_horizon+1))
```

### Specify and train models

Please specify models and train them on the train part:

```{r}
fit <- train |> model(
  # write your code here
)
```

### Produce forecasts

Here we need to first prepare the future values of predictors corresponding to the test set:

#### Values of population and strike in the test set

```{r}
fcs_ets_population <- train |> model(ets=ETS(???)) |> forecast(h=forecast_horizon)

population_forecast <- fcs_ets_population |> as_tibble() |> select(.mean)

test_future <- bind_cols(test,population_forecast) |>
  mutate(dose_adminstrated=.mean) |> 
  select(-.mean,-dose_adminstrated)

```

#### Produce forecasts for dose adminstrated

Complete the following code to generate forecast for the test set:

```{r}
forecast_vaccine <- ??? |> 
  ???(??? = ???)
```

You can visualise your forecast and see how the forecast looks like visually for the 12 months we predicted:

```{r}
forecast_vaccine |> 
  autoplot(filter_index(???, "2020" ~ .), level=NULL)
```

### Compute forecast accuracy

Let's compare the forecast accuracy of all models. Complete the R code to compute the point forecast accuracy, prediction interval accuracy and probabilistic distribution accuracy measures:

```{r}
??? |> 
  ???(???,
           measures = list(???,
                           ???,
                           ???
))
```

Which error metric do you use to evaluate the performance of models? Which model has the lowest error for each region? discuss your observation.

## Advanced performance evaluation

### Time series cross validation

> Attention: Depending on the number of time series, the computation time might be substantial and could potentially lead to computational delays.\]

This is also called rolling forecast or rolling origin: You can also reflect on the following questions:

-   Why do we use TSCV? you can read more [her](https://otexts.com/fpp3/tscv.html)

-   How do we do TSCV in R? Which steps to follow?

    1.  split data using `filter_index()` or other functions
    2.  create different time series origins
    3.  model each origin,
    4.  forecast each origin

let's see how we do it in R:

#### split data

We initially split the data into test and train. We defined a new variable, `percentage_test`. This will determine the percentage of the time series we use to evaluate the forecast accuracy using TSCV. As a general rule, we use 20%-30% of the length of time series as the test set. For instance, if we have 120 months of data, and use 20% as test set, that means we will have 24 months (120\*0.2) in the test set:

```{r}
forecast_horizon <- 12# forecast horizon
percentage_test <- 0.2 #20% of time series for test set

test <- vaccine_administrated_tsb |> 
  filter_index(as.character(max(vaccine_administrated_tsb$month)-round(percentage_test*length(unique(vaccine_administrated_tsb$month)))+1) ~ .)

train <- vaccine_administrated_tsb |>
  filter_index(. ~ as.character(max(vaccine_administrated_tsb$month)-(round(percentage_test*length(unique(vaccine_administrated_tsb$month))))))
```

#### Time series cross validation

Before fitting the models, we need to create the time series origins in both train and test sets. We first apply time series cross validation on the train data. We start with an initial training, the length of the first origin (.init = ) and then increase the length of the previous origin by adding new observation (.step=), we continue creating these timeseries until the number of observation left at the end of timeseries equals to the forecast horizon, we stop there.

Next, we apply time series cross validation on the test data. We create slides in the test set that corresponds to each origin created using train data, equal to the length of the forecast horizon.

```{r}
train_tscv <- vaccine_administrated_tsb |> 
  filter_index(. ~ as.character(max(vaccine_administrated_tsb$month)-(forecast_horizon))) |>
  ---(.init = length(unique(train$month)), .step = 1) # split data into different time series (i.e. origin or id) with increasing size

# you need also to get future values that correspond to each .id, because you need them in the forecast model:
test_tscv <- test |> 
  ---(.size = ---, .step = 1, .id = ".id") |> select(-dose_adminstrated)
```

> `.init` is the size of first origin, `.step` is the increment step, this can correspond to the forecasting frequency, i.e. how often you generate the forecast. If .step = 1 in a monthly time series, it means we generate forecasts very month for the given forecast horizon.

#### Values of population and strike in the test set

> It is important to replace `population_under1` values in the test_tscv with its estimation, otherwise we use perfect forecast for the population_under1 in the models using those predictors which can mislead us in choosing the most accurate model.

We don't have access to these forecast, so here we forecast them using ETS. Complete the R code to produce the estimation of population_under1 abd replace it with actual values in `test_tscv`:

```{r}
fcs_ets_population_tscv <- ??? |> 
  model(ets=ETS(???)) |> 
  forecast(h=forecast_horizon)

population_forecast_tscv <- fcs_ets_population_tscv |> as_tibble() |> select(.mean)

test_future_tscv <- bind_cols(test_tscv,population_forecast_tscv) |>
  mutate(dose_adminstrated=.mean) |> 
  select(-.mean)
```

#### specify and train models

We can train time series cross validation time series with regression models and any other models, this is exactly like what we have done before.

Complete the R code to train all models on the TSCV data:

```{r}
fit_tscv <- ??? |> 
  model(
    #wrire your code here
  )
fit_tscv
```

Observe the `fit_tscv` object.

What type of data structure is it? How many rows and columns are present, and what do they represent?

#### produce forecasts

We can forecast using trained models above. Complete the R code to produce forecasts for the TSCV:

```{r}
fcst_tscv <- ??? |> 
  forecast(??? = ???)
fcst_tscv
```

Observe the `fcst_tscv` object.

What type of data structure is it? How many rows and columns are present, and what do they represent?

#### forecast accuracy

Let's compare the forecast accuracy of all models. Complete the R code to compute the point forecast accuracy, prediction interval accuracy and probabilistic distribution accuracy measures:

```{r}
fcst_accuracy <- ??? |> 
  ???(???,
           ??? = ???(???,
                     ???,
                     ???
)) 

fcst_accuracy
```

Observe the `fcst_accuracy` object.

What type of data structure is it? How many rows and columns are present, and what do they represent?

You may want to select measures you focus on:

```{r}
fcst_accuracy |> select(RMSE,MAE,MASE, winkler, CRPS)
```

You can calculate the overall accuracy (using any error measure) across all regions:

```{r}
fcst_accuracy |> group_by(.model) |> summarise(MASE=mean(MASE), winkler=mean(winkler), CRPS=mean(CRPS)) |> arrange(MASE)
```

This will provide an overall summary (i.e an average) of multiple accuracy measures across all origins and forecast horizon. The result is summarised automatically across all origins (.id) and horizon using a simple average.

Which method is the best method (i.e. lowest error metric)?

#### accuracy per id

Now let's see how we can get the accuracy measure for each origin (i.e. .id) separately instead of averaging across all of them. To do this, you need to use an additional argument in accuracy(by=):

```{r}
fc_accuracy_by_id <-  |> 
  accuracy(???, by = c("???","???","???"))

```

We can now create some insightful visualisations. Complete the following code to generate a density plot and a box plot that highlights the distribution of the error metrics. You can choose any error metric:

```{r}
# Density plot
??? |> 
  select(.id,.model,???) |> 
  ggplot(aes(???))+
    geom_density(aes(fill=factor(.model)), alpha=.5)
```

```{r}
#Boxplot
??? |> 
  select(.id,.model,???) |> 
ggplot(aes(y= fct_reorder(.model,???), x=???))+
    geom_boxplot()
```

What insights do these plots provide?

#### accuracy across horizon

What if you want to show the accuracy measure for each model and each horizon (h=1, 2,...,12)?

In fable we don't get automatically a column that corresponds to forecast horizon (h=1,2,3,..., 12). If this is something you are interested in, you can do it yourself, let's first observe the first 24 observations to see the difference later:

```{r}
View(fcst_tscv[1:24,])
```

We first need to group by `id` and `.model` and then create a new variable called `h` and assign `row_number()` to it (you can type ?row_number in your Console to see what this function does, it simply returns the number of row):

```{r}
fc_h <- fcst_tscv |> 
  group_by(.id,.model,region) |> 
  mutate(h=row_number()) |> ungroup()
View(fc_h[1:24,])# view the first 24 rows of ae_fc and observe h
```

Now check rows from 12 to 24 to see the difference.

To calculate the accuracy measures for each horizon and model, complete the following code :

```{r}
fc_accuracy_h <- fc_h |> 
  as_fable(response = "dose_adminstrated", distribution = "dose_adminstrated") |> 
accuracy(???, 
           measures = list(point_accuracy_measures, 
                           interval_accuracy_measures, 
                           distribution_accuracy_measures),
           by = c("???","???","???"))

```

You can now create a line chart to show how forecast accuracy may change over the forecast horizon. Please complete the R code for a metric of your preference. You can replicate this process by changing the chosen metric:

```{r}
ggplot(data = fc_accuracy_h, 
       mapping = aes(x = ???, y = ???, color = .model))+
         geom_point()+
  geom_line()+
  ggthemes::scale_color_colorblind()+
ggthemes::theme_clean()
```

What insights do these plots provide?

### Forecast using best model for the future and visualise it

Now, we need to generate forecast for the future of the time series using the best model identified above. In order to do that, we need to get the values of predictors in the future corresponding to the forecast horizon, we first need to use `new_data()` followed by some data manipulation to get the new data required for forecasting:

You first need to produce the future months. Complete the following code todo that:

```{r}
future_month <- new_data(vaccine_administrated_tsb, n=forecast_horizon)
```

We assume that we know that in March the country will face strikes. Add a new column, *strike* by completing the R code:

```{r}
future_month_strike <- future_month |> 
  mutate(strike = if_else )
```

Add a new column, `population_under1`, to include the estimated population under 1, by completing the R code:

```{r}
forecast_population <- vaccine_administrated_tsb |> model(regression_population=ETS(???)) |> forecast(h=???)

population_point_forecast <- forecast_population |> as_tibble() |> select(.mean)

test_future <- bind_cols(future_month_strike, population_point_forecast) |>
  mutate(population_under1=.mean) |> select(-.mean,-dose_adminstrated)

```

Train the combination approach on the entire time series data:

```{r}
fit_future <- vaccine_administrated_tsb |> 
  model(???) |> 
  mutate(combination = (automatic_ets+automatic_arima+regression_population_strike)/3
) 
```

Forecast using the combination approach for the future:

```{r}
fcst_future <- ??? |> 
  forecast(new_data = test_future)
```

```{r}
fcst_future |> autoplot(filter_index(???, "2020" ~ .))# visualise it
```

### Residual diagnostics

Now, let's perform the residual diagnostic for the most accurate forecasts identified above through time series cross validation.

Plot the residuals:

```{r}
fit_future |> ???() |> filter(region == "") |> 
autoplot(???) +
  labs(title = "Residuals from the from the most accurate model")
```

Create the histogram of residuals:

```{r}
fit_future |> ???() |> filter(region == "") |> 
  ggplot(aes(x = ???)) +
  ???() +
  labs(title = "Histogram of residuals from the most accurate model")
```

Create the ACF plot of residuals:

```{r}
fit_future |> ???() |> filter(region == "") |> 
  ???(???) |>
  autoplot() +
  labs(title = "Residuals from the most accurate model")
```

Instead, you could use a function that provides all three plots together:

```{r}
fit_future |> filter(region == "") |> 
  ???()
```

What does the analysis of residuals reveal about the best model? Are there any systematic patterns left in the residuals?
